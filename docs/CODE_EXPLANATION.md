# ã‚³ãƒ¼ãƒ‰ã®è§£èª¬
## ãƒšãƒ¼ã‚¸ã®è¡¨ç¤ºã®é †ç•ª
1. ã‚¿ã‚¤ãƒˆãƒ«
2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
3. ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›

## ã‚¿ã‚¤ãƒˆãƒ«ã®è¡¨ç¤º
```Python
st.title("ğŸ¤– Streamlitã¨LangChainã«ã‚ˆã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
```

## ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã®è¡¨ç¤º
### è¡¨ç¤ºã™ã‚‹å‰ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ãŒãªã‘ã‚Œã°åˆæœŸåŒ–
```Python
if "messages" not in st.session_state:
    st.session_state.messages = []  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’ä¿æŒã™ã‚‹ãƒªã‚¹ãƒˆ
```

### ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’è¡¨ç¤ºã™ã‚‹
```Python
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
```

## ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®è¡¨ç¤ºã¨å¿œç­”
ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã§å…¥åŠ›æ¬„ãŒè¡¨ç¤ºã•ã‚Œã‚‹
```Python
st.chat_input("AIã«èããŸã„ã“ã¨ã‚’æ›¸ã„ã¦ã­")
```

### å…¨ä½“
```Python
if user_input := st.chat_input("AIã«èããŸã„ã“ã¨ã‚’æ›¸ã„ã¦ã­"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’Streamlitã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã«è¿½åŠ ã—ã€è¡¨ç¤º
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # å‡¦ç†ä¸­ã®ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ã‚’è¡¨ç¤º
    with st.spinner("AIãŒè€ƒãˆã¦ã„ã¾ã™..."):
        # Streamlitã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’LangChainå½¢å¼ã«å¤‰æ›ï¼ˆæœ€æ–°ã®å…¥åŠ›ã¯é™¤å¤–ï¼‰
        history = messages_to_langchain(st.session_state.messages[:-1])
        # AIå¿œç­”ã‚’å–å¾—
        response = get_response(chain, history, user_input)

    # å¿œç­”ã‚’Streamlitã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã«è¿½åŠ ã—ã€è¡¨ç¤º
    st.session_state.messages.append({"role": "assistant", "content": response})
    with st.chat_message("assistant"):
        st.markdown(response)
```

### Streamlitã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’LangChainå½¢å¼ã«å¤‰æ›ã™ã‚‹é–¢æ•°
```Python
def messages_to_langchain(messages):
    """Streamlitã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’LangChainã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›ã™ã‚‹"""
    langchain_messages = []
    for msg in messages:
        if msg["role"] == "user":
            langchain_messages.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "assistant":
            langchain_messages.append(AIMessage(content=msg["content"]))
    return langchain_messages
```

### LLMã®å¿œç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•°
```Python
def get_response(chain, history, user_input):
    """LangChainã®ãƒã‚§ã‚¤ãƒ³ã‚’å‘¼ã³å‡ºã—ã¦AIã®å¿œç­”ã‚’å–å¾—ã™ã‚‹"""
    try:
        response = chain.invoke({
            "history": history,  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
            "input": user_input  # æœ€æ–°ã®å…¥åŠ›
        })
        # Gemma 3å¯¾å¿œ: <0xE3><0x80><0x80>ã‚’å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›ã—ã¦è¿”ã™
        return response.replace("\u3000", "ã€€")
    except ConnectionError:
        return "âš ï¸ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    except Exception as e:
        return f"âš ï¸ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
```

### LLMã®è¨­å®š
```Python
llm = ChatOpenAI(
    model_name="gemma3",
    openai_api_base="http://localhost:11434/v1",
    openai_api_key="ollama",
    temperature=0.6,  # å¿œç­”ã®å‰µé€ æ€§ã‚’èª¿æ•´
)
```
### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å®šç¾©
```Python
prompt = ChatPromptTemplate.from_messages([
    ("system", "ã‚ãªãŸã¯å½¹ã«ç«‹ã¤ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯„ã‚Šæ·»ã£ã¦è³ªå•ã«ã‚„ã•ã—ãã¦ã„ã­ã„ã«ç­”ãˆã¦ãã ã•ã„ã€‚"),
    ("placeholder", "{history}"),  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
    ("human", "{input}"),          # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€æ–°å…¥åŠ›
])
```
### ãƒã‚§ã‚¤ãƒ³ã®å®šç¾©ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â†’ LLM â†’ å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼ï¼‰
```Python
chain = prompt | llm | StrOutputParser()
```
